{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.791319Z","iopub.status.idle":"2025-07-20T17:38:57.791523Z","shell.execute_reply.started":"2025-07-20T17:38:57.791426Z","shell.execute_reply":"2025-07-20T17:38:57.791435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Codemonk Machine Learning Intern Assignment**","metadata":{}},{"cell_type":"markdown","source":"# Fashion Product Image Classifier","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.792775Z","iopub.status.idle":"2025-07-20T17:38:57.792980Z","shell.execute_reply.started":"2025-07-20T17:38:57.792876Z","shell.execute_reply":"2025-07-20T17:38:57.792885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = Path('/kaggle/input/fashion-product-images-dataset/fashion-dataset')\nIMAGE_DIR = BASE_DIR / 'images'\nMETADATA_FILE = BASE_DIR / 'styles.csv'\n\n# Load metadata\nmetadata_df = pd.read_csv(METADATA_FILE, on_bad_lines='skip')\nmetadata_df = metadata_df.dropna(subset=['id', 'baseColour', 'articleType', 'season', 'gender'])\n\n# Add image paths\nmetadata_df['image_path'] = metadata_df['id'].apply(lambda x: os.path.join(IMAGE_DIR, f\"{x}.jpg\"))\nmetadata_df = metadata_df[metadata_df['image_path'].apply(os.path.exists)].reset_index(drop=True)\n\nprint(\"[INFO] Cleaned metadata shape:\", metadata_df.shape)\nmetadata_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.794042Z","iopub.status.idle":"2025-07-20T17:38:57.794350Z","shell.execute_reply.started":"2025-07-20T17:38:57.794192Z","shell.execute_reply":"2025-07-20T17:38:57.794206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"metadata_df.info()\nmetadata_df.describe(include='all')\nmetadata_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.795834Z","iopub.status.idle":"2025-07-20T17:38:57.796130Z","shell.execute_reply.started":"2025-07-20T17:38:57.795976Z","shell.execute_reply":"2025-07-20T17:38:57.795990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display available columns\nprint(\"[INFO] Available columns:\")\nprint(metadata_df.columns.tolist())\n\n# Columns to check for unique values\ncategory_columns = ['subCategory', 'articleType', 'productType']\n\n# Loop through each column and show top 10 value counts\nfor col in category_columns:\n    if col in metadata_df.columns:\n        print(f\"\\n[INFO] Top unique values in '{col}':\")\n        print(metadata_df[col].value_counts().head(10))\n    else:\n        print(f\"\\n[WARNING] Column '{col}' not found in the DataFrame.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.796745Z","iopub.status.idle":"2025-07-20T17:38:57.796982Z","shell.execute_reply.started":"2025-07-20T17:38:57.796880Z","shell.execute_reply":"2025-07-20T17:38:57.796890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gender distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=metadata_df, x='gender', order=metadata_df['gender'].value_counts().index, palette='pastel')\nplt.title('ðŸ§‘â€ðŸ¤â€ðŸ§‘ Distribution of Gender')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.xticks(rotation=30)\nplt.show()\n\n# Season distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=metadata_df, x='season', order=metadata_df['season'].value_counts().index, palette='viridis')\nplt.title('â›… Distribution of Seasons')\nplt.xlabel('Season')\nplt.ylabel('Count')\nplt.xticks(rotation=30)\nplt.show()\n\n# BaseColour: Top 10 colours\ntop_colours = metadata_df['baseColour'].value_counts().nlargest(10)\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_colours.index, y=top_colours.values, palette='plasma')\nplt.title('ðŸŽ¨ Top 10 Base Colours')\nplt.xlabel('Colour')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n# ProductDisplayName: Top 10 types\ntop_products = metadata_df['productDisplayName'].value_counts().nlargest(10)\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_products.index, y=top_products.values, palette='coolwarm')\nplt.title('ðŸ‘• Top 10 Product Types')\nplt.xlabel('Product Type')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.797528Z","iopub.status.idle":"2025-07-20T17:38:57.797757Z","shell.execute_reply.started":"2025-07-20T17:38:57.797641Z","shell.execute_reply":"2025-07-20T17:38:57.797654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encode Labels","metadata":{}},{"cell_type":"code","source":"label_encoders = {\n    'baseColour': LabelEncoder(),\n    'articleType': LabelEncoder(),\n    'season': LabelEncoder(),\n    'gender': LabelEncoder()\n}\n\nfor col, le in label_encoders.items():\n    metadata_df[f'{col}_label'] = le.fit_transform(metadata_df[col].astype(str))\n\n# Optional: filter rare labels\nmin_samples = 3\ncounts = metadata_df['articleType_label'].value_counts()\nvalid_classes = counts[counts >= min_samples].index\nmetadata_df = metadata_df[metadata_df['articleType_label'].isin(valid_classes)].reset_index(drop=True)\n\nprint(\"[INFO] Final dataset shape:\", metadata_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.798513Z","iopub.status.idle":"2025-07-20T17:38:57.798827Z","shell.execute_reply.started":"2025-07-20T17:38:57.798655Z","shell.execute_reply":"2025-07-20T17:38:57.798670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train/Val/Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Step 1: Remove classes with fewer than 3 samples (so that even after splitting they have >=1 sample in all sets)\nlabel_counts = metadata_df['articleType_label'].value_counts()\nvalid_labels = label_counts[label_counts >= 3].index  # 3 ensures at least 1 sample in val and test\nmetadata_df = metadata_df[metadata_df['articleType_label'].isin(valid_labels)]\n\n# Step 2: Stratified split train vs (val+test)\ntrain_df, temp_df = train_test_split(\n    metadata_df,\n    test_size=0.2,\n    stratify=metadata_df['articleType_label'],\n    random_state=42\n)\n\n# Step 3: Stratified split val vs test\n# IMPORTANT: Use only those classes that still have at least 2 samples in temp_df\ntemp_counts = temp_df['articleType_label'].value_counts()\nvalid_temp_labels = temp_counts[temp_counts >= 2].index\ntemp_df = temp_df[temp_df['articleType_label'].isin(valid_temp_labels)]\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    stratify=temp_df['articleType_label'],\n    random_state=42\n)\n\nprint(f\"[INFO] Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.799762Z","iopub.status.idle":"2025-07-20T17:38:57.800070Z","shell.execute_reply.started":"2025-07-20T17:38:57.799907Z","shell.execute_reply":"2025-07-20T17:38:57.799920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and Transforms","metadata":{}},{"cell_type":"code","source":"class FashionDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(row['image_path']).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n\n        labels = {\n            'colour': row['baseColour_label'],\n            'product_type': row['articleType_label'],\n            'season': row['season_label'],\n            'gender': row['gender_label']\n        }\n        return img, labels\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_ds = FashionDataset(train_df, transform)\nval_ds = FashionDataset(val_df, transform)\ntest_ds = FashionDataset(test_df, transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.801441Z","iopub.status.idle":"2025-07-20T17:38:57.801812Z","shell.execute_reply.started":"2025-07-20T17:38:57.801613Z","shell.execute_reply":"2025-07-20T17:38:57.801647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model - Use EfficientNet","metadata":{}},{"cell_type":"code","source":"from torchvision.models import efficientnet_b0\n\nclass MultiOutputEfficientNet(nn.Module):\n    def __init__(self, n_colours, n_product_types, n_seasons, n_genders):\n        super().__init__()\n        self.backbone = efficientnet_b0(pretrained=True)\n        self.backbone.classifier = nn.Identity()\n        self.fc_colour = nn.Linear(1280, n_colours)\n        self.fc_product_type = nn.Linear(1280, n_product_types)\n        self.fc_season = nn.Linear(1280, n_seasons)\n        self.fc_gender = nn.Linear(1280, n_genders)\n\n    def forward(self, x):\n        feat = self.backbone(x)\n        return {\n            'colour': self.fc_colour(feat),\n            'product_type': self.fc_product_type(feat),\n            'season': self.fc_season(feat),\n            'gender': self.fc_gender(feat)\n        }\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MultiOutputEfficientNet(\n    n_colours=len(label_encoders['baseColour'].classes_),\n    n_product_types=len(label_encoders['articleType'].classes_),\n    n_seasons=len(label_encoders['season'].classes_),\n    n_genders=len(label_encoders['gender'].classes_)\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.802787Z","iopub.status.idle":"2025-07-20T17:38:57.803132Z","shell.execute_reply.started":"2025-07-20T17:38:57.802951Z","shell.execute_reply":"2025-07-20T17:38:57.802967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for imgs, labels in tqdm(loader):\n        imgs = imgs.to(device)\n        targets = {k: torch.tensor(v).to(device) for k, v in labels.items()}\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = sum(criterion(outputs[k], targets[k]) for k in outputs)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.804173Z","iopub.status.idle":"2025-07-20T17:38:57.804489Z","shell.execute_reply.started":"2025-07-20T17:38:57.804327Z","shell.execute_reply":"2025-07-20T17:38:57.804341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{}},{"cell_type":"code","source":"def evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, correct = 0, 0\n    total = 0\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs = imgs.to(device)\n            targets = {k: torch.tensor(v).to(device) for k, v in labels.items()}\n            outputs = model(imgs)\n            loss = sum(criterion(outputs[k], targets[k]) for k in outputs)\n            total_loss += loss.item()\n            total += imgs.size(0)\n    return total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.805540Z","iopub.status.idle":"2025-07-20T17:38:57.805857Z","shell.execute_reply.started":"2025-07-20T17:38:57.805678Z","shell.execute_reply":"2025-07-20T17:38:57.805691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"EPOCHS = 4\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss = evaluate(model, val_loader, criterion, device)\n    print(f\"[EPOCH {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.807094Z","iopub.status.idle":"2025-07-20T17:38:57.807383Z","shell.execute_reply.started":"2025-07-20T17:38:57.807243Z","shell.execute_reply":"2025-07-20T17:38:57.807256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Encoders & Model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'best_model.pth')\n\nfor name, le in label_encoders.items():\n    with open(f'le_{name}.pkl', 'wb') as f:\n        pickle.dump(le, f)\n\nprint(\"âœ… Model and encoders saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.809250Z","iopub.status.idle":"2025-07-20T17:38:57.809509Z","shell.execute_reply.started":"2025-07-20T17:38:57.809394Z","shell.execute_reply":"2025-07-20T17:38:57.809407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference on New Image","metadata":{}},{"cell_type":"code","source":"def predict_image(img_path, model, device):\n    model.eval()\n    img = Image.open(img_path).convert('RGB')\n    img = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(img)\n        preds = {\n            'colour': label_encoders['baseColour'].inverse_transform([output['colour'].argmax(1).item()])[0],\n            'type': label_encoders['articleType'].inverse_transform([output['product_type'].argmax(1).item()])[0],\n            'season': label_encoders['season'].inverse_transform([output['season'].argmax(1).item()])[0],\n            'gender': label_encoders['gender'].inverse_transform([output['gender'].argmax(1).item()])[0]\n        }\n    return preds\n\n# Test on sample image\nsample_imgs = glob.glob('/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/*.jpg')[:5]\n\nfor img_path in sample_imgs:\n    preds = predict_image(img_path, model, device)\n    img = Image.open(img_path)\n    plt.imshow(img)\n    plt.title(str(preds))\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:38:57.810481Z","iopub.status.idle":"2025-07-20T17:38:57.810820Z","shell.execute_reply.started":"2025-07-20T17:38:57.810649Z","shell.execute_reply":"2025-07-20T17:38:57.810666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\n1.I conducted EDA and preprocessing with Python to prepare data.\n2.Built a multi-output classifier for color, type, season, and gender predictions.\n3.Applied model to Amazon images.","metadata":{}}]}